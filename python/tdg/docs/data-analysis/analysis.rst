Analysis
========

We usually are not interested in the valued of observables on each configuration, but wish to compute *expectation values*.

Thermalization
--------------

To get an unbiased estimate from a :class:`~.GrandCanonical` ensemble that was generated as part of a :class:`~.MarkovChain` we have to guarantee that the set of configurations which we account for in our expectation values do not remember the initial state the chain was started from, as that state is generated by some process that is *not* representative of the actual distribution of interest (in a :func:`'hot' <tdg.ensemble.GrandCanonical.generate>` start, it is from the quenched distribution, for example).

Knowing how many configurations to cut is a judgement call, and you may be misled if the observables you consider thermalize very quickly; not every observable need thermalize at once.  Moreover, one can never be completely confident that your samples are drawn from the basin of lowest action (in a global sense); perhaps the Markov chain simply has not reached a preferable basin on configurations.

To facilitate measuring only on configurations after a certain step in the Markov chain, the :class:`~.GrandCanonical` ensemble provides the :func:`cut <tdg.ensemble.GrandCanonical.cut>` method, which returns another :class:`~.GrandCanonical` ensemble without the leading configurations.



Handling Autocorrelation
------------------------

For a :class:`GrandCanonical` ensemble that was generated as part of a :class:`~.MarkovChain` there are correlations from one configuration to the next.
This introduces `autocorrelation`_ into the observables time series; a naive expectation value that does not account for the autocorrelation will produce underestimated uncertainties.
Good algorithms for estimating the *autocorrelation time* are known :cite:`Wolff:2003sm`.

Some simple ways of decreasing autocorrelation are to decimate your Markov Chain, only keeping every nᵗʰ configuration.  The :class:`~.GrandCanonical` ensemble provides the :func:`every <tdg.ensemble.GrandCanonical.every>` method, which returns another :class:`~.GrandCanonical` ensemble keeping configurations evenly spaced by n.

Another strategy is to *bin*: average observable values over consecutive configurations.  Because :code:`bin` is a `python built-in`_ we avoid methods with that name.

.. autoclass:: tdg.analysis.Binning
   :members:
   :undoc-members:
   :show-inheritance:

.. _autocorrelation: https://en.wikipedia.org/wiki/Autocorrelation
.. _python built-in: https://docs.python.org/3/library/functions.html?highlight=bin#bin
